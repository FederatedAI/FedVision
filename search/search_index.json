{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"FedVision is a Visual Object Detection Platform Powered by Federated Learning At this version, FedVision utilize PaddleFL and PaddleDetection to achieve Gradient Average strategy. This project is in really early stage but we aim to implement many awesome functions such as google's secure aggregation protocol and Gradient compression .","title":"Welcome to Fedvision"},{"location":"apis/cluster/","text":"manager \u00b6 fedvision.framework.cluster.manager \u00b6 ClusterManager \u00b6 __init__ ( self , port , host = None ) special \u00b6 init cluster manager instance Parameters: Name Type Description Default port int required host str None Source code in fedvision/framework/cluster/manager.py def __init__ ( self , port : int , host : str = None , ): \"\"\" init cluster manager instance Args: port: host: \"\"\" self . _host = \"[::]\" if host is None else host self . _port = port self . _alive_workers : MutableMapping [ str , _WorkerDescription ] = {} self . _tasks_status = {} self . _max_heartbeat_delay = 5 self . _server : Optional [ grpc . aio . Server ] = None add_worker ( self , worker_id , worker_ip , max_tasks , port_start , port_end ) \u00b6 add worker to manager Parameters: Name Type Description Default worker_id required worker_ip required max_tasks required port_start required port_end required Source code in fedvision/framework/cluster/manager.py def add_worker ( self , worker_id , worker_ip , max_tasks , port_start , port_end ): \"\"\" add worker to manager Args: worker_id: worker_ip: max_tasks: port_start: port_end: Returns: \"\"\" worker = _WorkerDescription ( worker_id = worker_id , worker_ip = worker_ip , max_tasks = max_tasks , max_delay = self . _max_heartbeat_delay , port_start = port_start , port_end = port_end , ) self . _alive_workers [ worker_id ] = worker async def _healthy_watcher (): try : while True : await asyncio . sleep ( self . _max_heartbeat_delay ) if worker_id not in self . _alive_workers : self . error ( f \"worker: { worker_id } not found\" ) break if worker . is_asystole (): self . error ( f \"heartbeat from worker: { worker_id } loss\" ) break finally : self . remove_worker ( worker_id ) asyncio . create_task ( _healthy_watcher ()) return worker dispatch ( self , resource = None ) async \u00b6 dispatch tasks to worker Parameters: Name Type Description Default resource dict None Returns: Type Description Tuple[Optional[_WorkerDescription], list] Source code in fedvision/framework/cluster/manager.py async def dispatch ( self , resource : dict = None ) -> Tuple [ Optional [ \"_WorkerDescription\" ], list ]: \"\"\" dispatch tasks to worker Args: resource: Returns: \"\"\" if resource is None : resource = {} if not resource : for k , v in self . _alive_workers . items (): if v . has_task_capacity (): v . task_task_capacity () return v , [] elif \"endpoints\" in resource : num_endpoints = resource [ \"endpoints\" ] for k , v in self . _alive_workers . items (): if v . has_num_valid_endpoints ( num_endpoints ) and v . has_task_capacity (): v . task_task_capacity () endpoints = v . take_endpoints ( num_endpoints ) return v , endpoints return None , [] Enroll ( self , request , context ) \u00b6 rpc server impl: process tasker enroll request Parameters: Name Type Description Default request REQ required context ServicerContext required Returns: Type Description AsyncGenerator[fedvision.framework.protobuf.cluster_pb2.REP, NoneType] Source code in fedvision/framework/cluster/manager.py async def Enroll ( self , request : cluster_pb2 . Enroll . REQ , context : grpc . aio . ServicerContext , ) -> AsyncGenerator [ cluster_pb2 . Enroll . REP , None ]: \"\"\" rpc server impl: process tasker enroll request Args: request: context: Returns: \"\"\" self . debug ( f \"cluster worker enroll request: { pretty_pb ( request ) } \" ) if self . has_worker ( request . worker_id ): yield cluster_pb2 . Enroll . REP ( status = cluster_pb2 . Enroll . ALREADY_ENROLL ) return worker = self . add_worker ( request . worker_id , request . worker_ip , request . max_tasks , request . port_start , request . port_end , ) self . debug ( f \"cluster worker enroll success: worker: { request . worker_id } \" ) yield cluster_pb2 . Enroll . REP ( status = cluster_pb2 . Enroll . ENROLL_SUCCESS ) while self . has_worker ( request . worker_id ): try : task = await worker . wait_next_task ( timeout = 5 ) except asyncio . TimeoutError : continue self . debug ( f \"task ready: job_id= { task . job_id } , task_id= { task . task_id } , task_type= { task . task_type } \" ) rep = cluster_pb2 . Enroll . REP ( status = cluster_pb2 . Enroll . TASK_READY , task = task ) self . debug ( f \"response task( { task . task_id } , { task . task_type } ) to worker { request . worker_id } \" ) yield rep self . remove_worker ( request . worker_id ) has_worker ( self , worker_id ) \u00b6 check worker worker_id alive(enrolled) Parameters: Name Type Description Default worker_id required Returns: Type Description bool Source code in fedvision/framework/cluster/manager.py def has_worker ( self , worker_id ) -> bool : \"\"\" check worker `worker_id` alive(enrolled) Args: worker_id: Returns: \"\"\" return worker_id in self . _alive_workers remove_worker ( self , worker_id ) \u00b6 remove worker from manager Parameters: Name Type Description Default worker_id required Source code in fedvision/framework/cluster/manager.py def remove_worker ( self , worker_id ): \"\"\" remove worker from manager Args: worker_id: \"\"\" if worker_id not in self . _alive_workers : return del self . _alive_workers [ worker_id ] start ( self ) async \u00b6 start cluster manager service Source code in fedvision/framework/cluster/manager.py async def start ( self ): \"\"\" start cluster manager service Returns: \"\"\" self . info ( f \"starting cluster manager at port: { self . _port } \" ) self . _server = grpc . aio . server ( options = [ ( \"grpc.max_send_message_length\" , 512 * 1024 * 1024 ), ( \"grpc.max_receive_message_length\" , 512 * 1024 * 1024 ), ], ) cluster_pb2_grpc . add_ClusterManagerServicer_to_server ( self , self . _server ) self . _server . add_insecure_port ( f \" { self . _host } : { self . _port } \" ) await self . _server . start () self . info ( f \"cluster manager started at port: { self . _port } \" ) stop ( self ) async \u00b6 stop cluster manager service Source code in fedvision/framework/cluster/manager.py async def stop ( self ): \"\"\" stop cluster manager service \"\"\" await self . _server . stop ( 1 ) TaskResourceRequire ( self , request , context ) async \u00b6 process task resource acquired request Parameters: Name Type Description Default request required context required Source code in fedvision/framework/cluster/manager.py async def TaskResourceRequire ( self , request , context ): \"\"\" process task resource acquired request Args: request: context: Returns: \"\"\" worker , endpoints = await self . dispatch ( resource = { \"endpoints\" : request . num_endpoints } ) if worker is None : return cluster_pb2 . TaskResourceRequire . REP ( status = cluster_pb2 . TaskResourceRequire . FAILED ) response = cluster_pb2 . TaskResourceRequire . REP ( status = cluster_pb2 . TaskResourceRequire . SUCCESS , worker_id = worker . worker_id ) for endpoint in endpoints : response . endpoints . append ( endpoint ) return response TaskSubmit ( self , request , context ) async \u00b6 process task submit request Parameters: Name Type Description Default request REQ required context ServicerContext required Returns: Type Description REP Source code in fedvision/framework/cluster/manager.py async def TaskSubmit ( self , request : cluster_pb2 . TaskSubmit . REQ , context : grpc . aio . ServicerContext ) -> cluster_pb2 . TaskSubmit . REP : \"\"\" process task submit request Args: request: context: Returns: \"\"\" try : task = request . task if not task . assignee : worker , _ = await self . dispatch () await worker . put_task ( task = task ) else : await self . _alive_workers [ task . assignee ] . put_task ( task = task ) return cluster_pb2 . TaskSubmit . REP ( status = cluster_pb2 . TaskSubmit . SUCCESS ) except Exception as e : self . exception ( f \"handle task submit failed: { e } \" ) return cluster_pb2 . TaskSubmit . REP ( status = cluster_pb2 . TaskSubmit . FAILED ) UpdateTaskStatus ( self , request , context ) async \u00b6 process task status update request Parameters: Name Type Description Default request REQ required context ServicerContext required Returns: Type Description REP Source code in fedvision/framework/cluster/manager.py async def UpdateTaskStatus ( self , request : cluster_pb2 . UpdateStatus . REQ , context : grpc . aio . ServicerContext ) -> cluster_pb2 . UpdateStatus . REP : \"\"\" process task status update request Args: request: context: Returns: \"\"\" if request . worker_id not in self . _alive_workers : return cluster_pb2 . UpdateStatus . REP ( status = cluster_pb2 . UpdateStatus . FAILED ) await self . _alive_workers [ request . worker_id ] . update_heartbeat () if not request . task_id : return cluster_pb2 . UpdateStatus . REP ( status = cluster_pb2 . UpdateStatus . SUCCESS ) if not request . task_id not in self . _tasks_status : return cluster_pb2 . UpdateStatus . REP ( status = cluster_pb2 . UpdateStatus . FAILED ) self . debug ( f \"update task status: { request . task_id } to { request . task_status } \" ) self . _tasks_status [ request . task_id ] = request . task_status return cluster_pb2 . UpdateStatus . REP ( status = cluster_pb2 . UpdateStatus . SUCCESS ) worker \u00b6 fedvision.framework.cluster.worker \u00b6 ClusterWorker \u00b6 __init__ ( self , worker_id , worker_ip , max_tasks , port_start , port_end , manager_address , data_dir = None ) special \u00b6 init cluster worker instance Parameters: Name Type Description Default worker_id str required worker_ip str required max_tasks int required port_start int required port_end int required manager_address str required data_dir str None Source code in fedvision/framework/cluster/worker.py def __init__ ( self , worker_id : str , worker_ip : str , max_tasks : int , port_start : int , port_end : int , manager_address : str , data_dir : str = None , ): \"\"\" init cluster worker instance Args: worker_id: worker_ip: max_tasks: port_start: port_end: manager_address: data_dir: \"\"\" self . _task_queue : asyncio . Queue = asyncio . Queue () self . _semaphore = asyncio . Semaphore ( max_tasks ) self . _worker_id = worker_id self . _worker_ip = worker_ip self . _manager_address = manager_address self . _max_tasks = max_tasks self . _port_start = port_start self . _port_end = port_end self . _heartbeat_interval = 1 self . _data_dir = data_dir self . _channel : Optional [ grpc . Channel ] = None self . _stub : Optional [ cluster_pb2_grpc . ClusterManagerStub ] = None self . _tasks : List [ asyncio . Future ] = [] self . _task_status : asyncio . Queue = asyncio . Queue () self . _stop_event = asyncio . Event () self . _asyncio_task_collection : Optional [ List [ asyncio . Task ]] = None start ( self ) async \u00b6 start worker enroll to manager start heartbeat loop start task exec loop process tasks Source code in fedvision/framework/cluster/worker.py async def start ( self ): \"\"\" start worker 1. enroll to manager 2. start heartbeat loop 3. start task exec loop 4. process tasks \"\"\" self . info ( f \"starting worker { self . _worker_id } \" ) self . info ( f \"staring grpc channel to cluster manager\" ) self . _channel = grpc . aio . insecure_channel ( self . _manager_address , options = [ ( \"grpc.max_send_message_length\" , 512 * 1024 * 1024 ), ( \"grpc.max_receive_message_length\" , 512 * 1024 * 1024 ), ], ) self . _stub = cluster_pb2_grpc . ClusterManagerStub ( self . _channel ) self . info ( f \"sending enroll request to cluster manager\" ) response_stream : AsyncIterable [ cluster_pb2 . Enroll . REP ] = self . _stub . Enroll ( cluster_pb2 . Enroll . REQ ( worker_id = self . _worker_id , worker_ip = self . _worker_ip , max_tasks = self . _max_tasks , port_start = self . _port_start , port_end = self . _port_end , ) ) first_response = True try : async for response in response_stream : if first_response : if response . status == cluster_pb2 . Enroll . ALREADY_ENROLL : raise FedvisionWorkerException ( f \"worker< { self . _worker_id } > already enrolled, use new name or remove it from manager\" ) if response . status != cluster_pb2 . Enroll . ENROLL_SUCCESS : raise FedvisionWorkerException ( f \"worker< { self . _worker_id } >enroll failed with unknown status: { response . status } \" ) self . info ( f \"worker< { self . _worker_id } >success enrolled to cluster manager\" ) async def _co_update_status (): while True : try : request = await asyncio . wait_for ( self . _task_status . get (), self . _heartbeat_interval ) except asyncio . TimeoutError : self . trace ( \"wait task status timeout. sending heartbeat request\" ) request = cluster_pb2 . UpdateStatus . REQ ( worker_id = self . _worker_id ) try : update_response = await self . _stub . UpdateTaskStatus ( request ) except grpc . aio . AioRpcError as _e : self . error ( f \"can't send heartbeat to manager, { _e } \" ) self . _stop_event . set () return if ( update_response . status != cluster_pb2 . UpdateStatus . SUCCESS ): self . error ( f \"update status failed, please check manager status\" ) self . info ( \"starting heartbeat loop\" ) self . _asyncio_task_collection = [ asyncio . create_task ( _co_update_status ()), ] self . info ( \"heartbeat loop started\" ) self . info ( f \"starting task execute loop\" ) self . _asyncio_task_collection . append ( asyncio . create_task ( self . _co_task_execute_loop ()) ) self . info ( f \"task execute loop started\" ) first_response = False continue # fetch tasks if response . status != cluster_pb2 . Enroll . TASK_READY : raise FedvisionWorkerException ( f \"expect status { cluster_pb2 . Enroll . TASK_READY } , got { response . status } \" ) self . trace_lazy ( f \"response < {{ response }} > got\" , response = lambda : pretty_pb ( response ) ) try : task_id = response . task . task_id task_type = response . task . task_type task_class = extensions . get_task_class ( task_type ) if task_class is None : self . error ( f \"task type { task_type } not found\" ) raise FedvisionExtensionException ( f \"task type { task_type } not found\" ) task = task_class . deserialize ( response . task ) await self . _task_queue . put ( task ) self . trace ( f \"put task in queue: task_id= { task_id } \" ) except FedvisionException as e : self . error ( f \"preprocess fetched task failed: { e } \" ) except Exception as e : self . exception ( e ) except grpc . aio . AioRpcError as e : self . error ( f \"gRPC error: can't connect with cluster manager, { e } \" ) self . _stop_event . set () stop ( self ) async \u00b6 stop worker Source code in fedvision/framework/cluster/worker.py async def stop ( self ): \"\"\" stop worker \"\"\" if self . _channel is not None : await self . _channel . close () self . _channel = None self . info ( f \"canceling unfinished asyncio tasks\" ) if self . _asyncio_task_collection is not None : for task in self . _asyncio_task_collection : if not task . done (): task . cancel () self . trace ( f \"canceled task { task } \" ) self . info ( f \"all unfinished asyncio tasks canceled\" ) wait_for_termination ( self ) async \u00b6 block until stop event was set Source code in fedvision/framework/cluster/worker.py async def wait_for_termination ( self ): \"\"\" block until stop event was set \"\"\" await self . _stop_event . wait () self . info ( f \"stop event set, stopping worker { self . _worker_id } \" )","title":"cluster"},{"location":"apis/cluster/#manager","text":"","title":"manager"},{"location":"apis/cluster/#fedvision.framework.cluster.manager","text":"","title":"manager"},{"location":"apis/cluster/#fedvision.framework.cluster.manager.ClusterManager","text":"","title":"ClusterManager"},{"location":"apis/cluster/#fedvision.framework.cluster.manager.ClusterManager.__init__","text":"init cluster manager instance Parameters: Name Type Description Default port int required host str None Source code in fedvision/framework/cluster/manager.py def __init__ ( self , port : int , host : str = None , ): \"\"\" init cluster manager instance Args: port: host: \"\"\" self . _host = \"[::]\" if host is None else host self . _port = port self . _alive_workers : MutableMapping [ str , _WorkerDescription ] = {} self . _tasks_status = {} self . _max_heartbeat_delay = 5 self . _server : Optional [ grpc . aio . Server ] = None","title":"__init__()"},{"location":"apis/cluster/#fedvision.framework.cluster.manager.ClusterManager.add_worker","text":"add worker to manager Parameters: Name Type Description Default worker_id required worker_ip required max_tasks required port_start required port_end required Source code in fedvision/framework/cluster/manager.py def add_worker ( self , worker_id , worker_ip , max_tasks , port_start , port_end ): \"\"\" add worker to manager Args: worker_id: worker_ip: max_tasks: port_start: port_end: Returns: \"\"\" worker = _WorkerDescription ( worker_id = worker_id , worker_ip = worker_ip , max_tasks = max_tasks , max_delay = self . _max_heartbeat_delay , port_start = port_start , port_end = port_end , ) self . _alive_workers [ worker_id ] = worker async def _healthy_watcher (): try : while True : await asyncio . sleep ( self . _max_heartbeat_delay ) if worker_id not in self . _alive_workers : self . error ( f \"worker: { worker_id } not found\" ) break if worker . is_asystole (): self . error ( f \"heartbeat from worker: { worker_id } loss\" ) break finally : self . remove_worker ( worker_id ) asyncio . create_task ( _healthy_watcher ()) return worker","title":"add_worker()"},{"location":"apis/cluster/#fedvision.framework.cluster.manager.ClusterManager.dispatch","text":"dispatch tasks to worker Parameters: Name Type Description Default resource dict None Returns: Type Description Tuple[Optional[_WorkerDescription], list] Source code in fedvision/framework/cluster/manager.py async def dispatch ( self , resource : dict = None ) -> Tuple [ Optional [ \"_WorkerDescription\" ], list ]: \"\"\" dispatch tasks to worker Args: resource: Returns: \"\"\" if resource is None : resource = {} if not resource : for k , v in self . _alive_workers . items (): if v . has_task_capacity (): v . task_task_capacity () return v , [] elif \"endpoints\" in resource : num_endpoints = resource [ \"endpoints\" ] for k , v in self . _alive_workers . items (): if v . has_num_valid_endpoints ( num_endpoints ) and v . has_task_capacity (): v . task_task_capacity () endpoints = v . take_endpoints ( num_endpoints ) return v , endpoints return None , []","title":"dispatch()"},{"location":"apis/cluster/#fedvision.framework.cluster.manager.ClusterManager.Enroll","text":"rpc server impl: process tasker enroll request Parameters: Name Type Description Default request REQ required context ServicerContext required Returns: Type Description AsyncGenerator[fedvision.framework.protobuf.cluster_pb2.REP, NoneType] Source code in fedvision/framework/cluster/manager.py async def Enroll ( self , request : cluster_pb2 . Enroll . REQ , context : grpc . aio . ServicerContext , ) -> AsyncGenerator [ cluster_pb2 . Enroll . REP , None ]: \"\"\" rpc server impl: process tasker enroll request Args: request: context: Returns: \"\"\" self . debug ( f \"cluster worker enroll request: { pretty_pb ( request ) } \" ) if self . has_worker ( request . worker_id ): yield cluster_pb2 . Enroll . REP ( status = cluster_pb2 . Enroll . ALREADY_ENROLL ) return worker = self . add_worker ( request . worker_id , request . worker_ip , request . max_tasks , request . port_start , request . port_end , ) self . debug ( f \"cluster worker enroll success: worker: { request . worker_id } \" ) yield cluster_pb2 . Enroll . REP ( status = cluster_pb2 . Enroll . ENROLL_SUCCESS ) while self . has_worker ( request . worker_id ): try : task = await worker . wait_next_task ( timeout = 5 ) except asyncio . TimeoutError : continue self . debug ( f \"task ready: job_id= { task . job_id } , task_id= { task . task_id } , task_type= { task . task_type } \" ) rep = cluster_pb2 . Enroll . REP ( status = cluster_pb2 . Enroll . TASK_READY , task = task ) self . debug ( f \"response task( { task . task_id } , { task . task_type } ) to worker { request . worker_id } \" ) yield rep self . remove_worker ( request . worker_id )","title":"Enroll()"},{"location":"apis/cluster/#fedvision.framework.cluster.manager.ClusterManager.has_worker","text":"check worker worker_id alive(enrolled) Parameters: Name Type Description Default worker_id required Returns: Type Description bool Source code in fedvision/framework/cluster/manager.py def has_worker ( self , worker_id ) -> bool : \"\"\" check worker `worker_id` alive(enrolled) Args: worker_id: Returns: \"\"\" return worker_id in self . _alive_workers","title":"has_worker()"},{"location":"apis/cluster/#fedvision.framework.cluster.manager.ClusterManager.remove_worker","text":"remove worker from manager Parameters: Name Type Description Default worker_id required Source code in fedvision/framework/cluster/manager.py def remove_worker ( self , worker_id ): \"\"\" remove worker from manager Args: worker_id: \"\"\" if worker_id not in self . _alive_workers : return del self . _alive_workers [ worker_id ]","title":"remove_worker()"},{"location":"apis/cluster/#fedvision.framework.cluster.manager.ClusterManager.start","text":"start cluster manager service Source code in fedvision/framework/cluster/manager.py async def start ( self ): \"\"\" start cluster manager service Returns: \"\"\" self . info ( f \"starting cluster manager at port: { self . _port } \" ) self . _server = grpc . aio . server ( options = [ ( \"grpc.max_send_message_length\" , 512 * 1024 * 1024 ), ( \"grpc.max_receive_message_length\" , 512 * 1024 * 1024 ), ], ) cluster_pb2_grpc . add_ClusterManagerServicer_to_server ( self , self . _server ) self . _server . add_insecure_port ( f \" { self . _host } : { self . _port } \" ) await self . _server . start () self . info ( f \"cluster manager started at port: { self . _port } \" )","title":"start()"},{"location":"apis/cluster/#fedvision.framework.cluster.manager.ClusterManager.stop","text":"stop cluster manager service Source code in fedvision/framework/cluster/manager.py async def stop ( self ): \"\"\" stop cluster manager service \"\"\" await self . _server . stop ( 1 )","title":"stop()"},{"location":"apis/cluster/#fedvision.framework.cluster.manager.ClusterManager.TaskResourceRequire","text":"process task resource acquired request Parameters: Name Type Description Default request required context required Source code in fedvision/framework/cluster/manager.py async def TaskResourceRequire ( self , request , context ): \"\"\" process task resource acquired request Args: request: context: Returns: \"\"\" worker , endpoints = await self . dispatch ( resource = { \"endpoints\" : request . num_endpoints } ) if worker is None : return cluster_pb2 . TaskResourceRequire . REP ( status = cluster_pb2 . TaskResourceRequire . FAILED ) response = cluster_pb2 . TaskResourceRequire . REP ( status = cluster_pb2 . TaskResourceRequire . SUCCESS , worker_id = worker . worker_id ) for endpoint in endpoints : response . endpoints . append ( endpoint ) return response","title":"TaskResourceRequire()"},{"location":"apis/cluster/#fedvision.framework.cluster.manager.ClusterManager.TaskSubmit","text":"process task submit request Parameters: Name Type Description Default request REQ required context ServicerContext required Returns: Type Description REP Source code in fedvision/framework/cluster/manager.py async def TaskSubmit ( self , request : cluster_pb2 . TaskSubmit . REQ , context : grpc . aio . ServicerContext ) -> cluster_pb2 . TaskSubmit . REP : \"\"\" process task submit request Args: request: context: Returns: \"\"\" try : task = request . task if not task . assignee : worker , _ = await self . dispatch () await worker . put_task ( task = task ) else : await self . _alive_workers [ task . assignee ] . put_task ( task = task ) return cluster_pb2 . TaskSubmit . REP ( status = cluster_pb2 . TaskSubmit . SUCCESS ) except Exception as e : self . exception ( f \"handle task submit failed: { e } \" ) return cluster_pb2 . TaskSubmit . REP ( status = cluster_pb2 . TaskSubmit . FAILED )","title":"TaskSubmit()"},{"location":"apis/cluster/#fedvision.framework.cluster.manager.ClusterManager.UpdateTaskStatus","text":"process task status update request Parameters: Name Type Description Default request REQ required context ServicerContext required Returns: Type Description REP Source code in fedvision/framework/cluster/manager.py async def UpdateTaskStatus ( self , request : cluster_pb2 . UpdateStatus . REQ , context : grpc . aio . ServicerContext ) -> cluster_pb2 . UpdateStatus . REP : \"\"\" process task status update request Args: request: context: Returns: \"\"\" if request . worker_id not in self . _alive_workers : return cluster_pb2 . UpdateStatus . REP ( status = cluster_pb2 . UpdateStatus . FAILED ) await self . _alive_workers [ request . worker_id ] . update_heartbeat () if not request . task_id : return cluster_pb2 . UpdateStatus . REP ( status = cluster_pb2 . UpdateStatus . SUCCESS ) if not request . task_id not in self . _tasks_status : return cluster_pb2 . UpdateStatus . REP ( status = cluster_pb2 . UpdateStatus . FAILED ) self . debug ( f \"update task status: { request . task_id } to { request . task_status } \" ) self . _tasks_status [ request . task_id ] = request . task_status return cluster_pb2 . UpdateStatus . REP ( status = cluster_pb2 . UpdateStatus . SUCCESS )","title":"UpdateTaskStatus()"},{"location":"apis/cluster/#worker","text":"","title":"worker"},{"location":"apis/cluster/#fedvision.framework.cluster.worker","text":"","title":"worker"},{"location":"apis/cluster/#fedvision.framework.cluster.worker.ClusterWorker","text":"","title":"ClusterWorker"},{"location":"apis/cluster/#fedvision.framework.cluster.worker.ClusterWorker.__init__","text":"init cluster worker instance Parameters: Name Type Description Default worker_id str required worker_ip str required max_tasks int required port_start int required port_end int required manager_address str required data_dir str None Source code in fedvision/framework/cluster/worker.py def __init__ ( self , worker_id : str , worker_ip : str , max_tasks : int , port_start : int , port_end : int , manager_address : str , data_dir : str = None , ): \"\"\" init cluster worker instance Args: worker_id: worker_ip: max_tasks: port_start: port_end: manager_address: data_dir: \"\"\" self . _task_queue : asyncio . Queue = asyncio . Queue () self . _semaphore = asyncio . Semaphore ( max_tasks ) self . _worker_id = worker_id self . _worker_ip = worker_ip self . _manager_address = manager_address self . _max_tasks = max_tasks self . _port_start = port_start self . _port_end = port_end self . _heartbeat_interval = 1 self . _data_dir = data_dir self . _channel : Optional [ grpc . Channel ] = None self . _stub : Optional [ cluster_pb2_grpc . ClusterManagerStub ] = None self . _tasks : List [ asyncio . Future ] = [] self . _task_status : asyncio . Queue = asyncio . Queue () self . _stop_event = asyncio . Event () self . _asyncio_task_collection : Optional [ List [ asyncio . Task ]] = None","title":"__init__()"},{"location":"apis/cluster/#fedvision.framework.cluster.worker.ClusterWorker.start","text":"start worker enroll to manager start heartbeat loop start task exec loop process tasks Source code in fedvision/framework/cluster/worker.py async def start ( self ): \"\"\" start worker 1. enroll to manager 2. start heartbeat loop 3. start task exec loop 4. process tasks \"\"\" self . info ( f \"starting worker { self . _worker_id } \" ) self . info ( f \"staring grpc channel to cluster manager\" ) self . _channel = grpc . aio . insecure_channel ( self . _manager_address , options = [ ( \"grpc.max_send_message_length\" , 512 * 1024 * 1024 ), ( \"grpc.max_receive_message_length\" , 512 * 1024 * 1024 ), ], ) self . _stub = cluster_pb2_grpc . ClusterManagerStub ( self . _channel ) self . info ( f \"sending enroll request to cluster manager\" ) response_stream : AsyncIterable [ cluster_pb2 . Enroll . REP ] = self . _stub . Enroll ( cluster_pb2 . Enroll . REQ ( worker_id = self . _worker_id , worker_ip = self . _worker_ip , max_tasks = self . _max_tasks , port_start = self . _port_start , port_end = self . _port_end , ) ) first_response = True try : async for response in response_stream : if first_response : if response . status == cluster_pb2 . Enroll . ALREADY_ENROLL : raise FedvisionWorkerException ( f \"worker< { self . _worker_id } > already enrolled, use new name or remove it from manager\" ) if response . status != cluster_pb2 . Enroll . ENROLL_SUCCESS : raise FedvisionWorkerException ( f \"worker< { self . _worker_id } >enroll failed with unknown status: { response . status } \" ) self . info ( f \"worker< { self . _worker_id } >success enrolled to cluster manager\" ) async def _co_update_status (): while True : try : request = await asyncio . wait_for ( self . _task_status . get (), self . _heartbeat_interval ) except asyncio . TimeoutError : self . trace ( \"wait task status timeout. sending heartbeat request\" ) request = cluster_pb2 . UpdateStatus . REQ ( worker_id = self . _worker_id ) try : update_response = await self . _stub . UpdateTaskStatus ( request ) except grpc . aio . AioRpcError as _e : self . error ( f \"can't send heartbeat to manager, { _e } \" ) self . _stop_event . set () return if ( update_response . status != cluster_pb2 . UpdateStatus . SUCCESS ): self . error ( f \"update status failed, please check manager status\" ) self . info ( \"starting heartbeat loop\" ) self . _asyncio_task_collection = [ asyncio . create_task ( _co_update_status ()), ] self . info ( \"heartbeat loop started\" ) self . info ( f \"starting task execute loop\" ) self . _asyncio_task_collection . append ( asyncio . create_task ( self . _co_task_execute_loop ()) ) self . info ( f \"task execute loop started\" ) first_response = False continue # fetch tasks if response . status != cluster_pb2 . Enroll . TASK_READY : raise FedvisionWorkerException ( f \"expect status { cluster_pb2 . Enroll . TASK_READY } , got { response . status } \" ) self . trace_lazy ( f \"response < {{ response }} > got\" , response = lambda : pretty_pb ( response ) ) try : task_id = response . task . task_id task_type = response . task . task_type task_class = extensions . get_task_class ( task_type ) if task_class is None : self . error ( f \"task type { task_type } not found\" ) raise FedvisionExtensionException ( f \"task type { task_type } not found\" ) task = task_class . deserialize ( response . task ) await self . _task_queue . put ( task ) self . trace ( f \"put task in queue: task_id= { task_id } \" ) except FedvisionException as e : self . error ( f \"preprocess fetched task failed: { e } \" ) except Exception as e : self . exception ( e ) except grpc . aio . AioRpcError as e : self . error ( f \"gRPC error: can't connect with cluster manager, { e } \" ) self . _stop_event . set ()","title":"start()"},{"location":"apis/cluster/#fedvision.framework.cluster.worker.ClusterWorker.stop","text":"stop worker Source code in fedvision/framework/cluster/worker.py async def stop ( self ): \"\"\" stop worker \"\"\" if self . _channel is not None : await self . _channel . close () self . _channel = None self . info ( f \"canceling unfinished asyncio tasks\" ) if self . _asyncio_task_collection is not None : for task in self . _asyncio_task_collection : if not task . done (): task . cancel () self . trace ( f \"canceled task { task } \" ) self . info ( f \"all unfinished asyncio tasks canceled\" )","title":"stop()"},{"location":"apis/cluster/#fedvision.framework.cluster.worker.ClusterWorker.wait_for_termination","text":"block until stop event was set Source code in fedvision/framework/cluster/worker.py async def wait_for_termination ( self ): \"\"\" block until stop event was set \"\"\" await self . _stop_event . wait () self . info ( f \"stop event set, stopping worker { self . _worker_id } \" )","title":"wait_for_termination()"},{"location":"apis/coordinator/","text":"Coordinator \u00b6 \u00b6 coordinator \u00b6 Coordinator \u00b6 __init__ ( self , port ) special \u00b6 init coordinator Parameters: Name Type Description Default port int coordinator serving port required Source code in fedvision/framework/coordinator/coordinator.py def __init__ ( self , port : int ): \"\"\" init coordinator Args: port: coordinator serving port \"\"\" self . _serving = True self . _enrolled : MutableMapping [ str , _TaskProviderForEnrolledParty ] = {} self . _proposals : MutableMapping [ str , _Proposal ] = {} self . _job_type_to_subscribes : MutableMapping [ str , MutableSet [ str ]] = {} self . _check_interval = 0.5 self . _count_id = 0 self . _grpc_port = port self . _grpc_server = None FetchTask ( self , request , context ) async \u00b6 handle task fetch gRPC request Parameters: Name Type Description Default request coordinator_pb2.FetchTask.REQ required context grpc.aio.ServicerContext required Returns: Type Description coordinator_pb2.Proposal.REP Source code in fedvision/framework/coordinator/coordinator.py async def FetchTask ( self , request : coordinator_pb2 . FetchTask . REQ , context : grpc . aio . ServicerContext ) -> coordinator_pb2 . Proposal . REP : \"\"\" handle task fetch gRPC request Args: request: context: Returns: \"\"\" if request . proposal_id not in self . _proposals : return coordinator_pb2 . FetchTask . REP ( status = coordinator_pb2 . FetchTask . NOT_FOUND ) if request . party_id not in self . _enrolled : return coordinator_pb2 . FetchTask . REP ( status = coordinator_pb2 . FetchTask . NOT_ALLOW ) proposal = self . _proposals [ request . proposal_id ] if proposal . open_period_finished . is_set (): return coordinator_pb2 . FetchTask . REP ( status = coordinator_pb2 . FetchTask . TIMEOUT ) proposal . add_responders ( request . party_id ) await proposal . open_period_finished . wait () if not proposal . goal_reached : return coordinator_pb2 . FetchTask . REP ( status = coordinator_pb2 . FetchTask . CANCELED ) self . info ( f \"chosen: { proposal . chosen . keys () } \" ) if request . party_id not in proposal . chosen : return coordinator_pb2 . FetchTask . REP ( status = coordinator_pb2 . FetchTask . RANDOM_OUT ) # accepted, finally! success_rep = coordinator_pb2 . FetchTask . REP ( status = coordinator_pb2 . FetchTask . READY , task = proposal . chosen [ request . party_id ], ) return success_rep Leave ( self , request , context ) async \u00b6 Missing associated documentation comment in .proto file. Source code in fedvision/framework/coordinator/coordinator.py async def Leave ( self , request , context ): if request . party_id not in self . _enrolled : return coordinator_pb2 . Leave . REP ( status = coordinator_pb2 . Leave . NOT_FOUND ) self . _enrolled [ request . party_id ] . closed = True await self . _enrolled [ request . party_id ] . queue . join () self . _enrolled . __delitem__ ( request . party_id ) return coordinator_pb2 . Leave . REP ( status = coordinator_pb2 . Leave . SUCCESS ) Proposal ( self , request , context ) async \u00b6 handle job proposal gRPC request Parameters: Name Type Description Default request coordinator_pb2.Proposal.REQ required context grpc.aio.ServicerContext required Returns: Type Description coordinator_pb2.Proposal.REP Source code in fedvision/framework/coordinator/coordinator.py async def Proposal ( self , request : coordinator_pb2 . Proposal . REQ , context : grpc . aio . ServicerContext ) -> coordinator_pb2 . Proposal . REP : \"\"\" handle job proposal gRPC request Args: request: context: Returns: \"\"\" uid = self . _generate_proposal_id ( request . job_id ) proposal = _Proposal . from_pb ( uid = uid , pb = request ) self . _proposals [ uid ] = proposal if proposal . job_type not in self . _job_type_to_subscribes : self . info ( f \"job type { proposal . job_type } not in { self . _job_type_to_subscribes } , reject\" ) return coordinator_pb2 . Proposal . REP ( status = coordinator_pb2 . Proposal . REJECT ) if ( len ( self . _job_type_to_subscribes [ proposal . job_type ]) < proposal . minimum_acceptance ): self . info ( f \"not enough parties alive accept job type { proposal . job_type } ,\" f \" required { proposal . minimum_acceptance } , \" f \" { len ( self . _job_type_to_subscribes [ proposal . job_type ]) } alive\" ) return coordinator_pb2 . Proposal . REP ( status = coordinator_pb2 . Proposal . NOT_ENOUGH_SUBSCRIBERS ) # dispatch proposal for party_id in self . _job_type_to_subscribes [ proposal . job_type ]: await self . _enrolled [ party_id ] . queue . put ( proposal ) # sleep until timeout and then check if there are enough responders await asyncio . sleep ( request . proposal_wait_time ) if not proposal . has_enough_responders (): proposal . set_open_period_finished ( goal_reached = False ) return coordinator_pb2 . Proposal . REP ( status = coordinator_pb2 . Proposal . NOT_ENOUGH_RESPONDERS ) proposal . set_open_period_finished ( goal_reached = True ) return coordinator_pb2 . Proposal . REP ( status = coordinator_pb2 . Proposal . SUCCESS ) Subscribe ( self , request , context ) \u00b6 handle subscribe gRPC request, response job proposals in stream Parameters: Name Type Description Default request coordinator_pb2.Subscribe.REQ required context grpc.aio.ServicerContext required Returns: Type Description AsyncGenerator[coordinator_pb2.Subscribe.REP, None] Source code in fedvision/framework/coordinator/coordinator.py async def Subscribe ( self , request : coordinator_pb2 . Subscribe . REQ , context : grpc . aio . ServicerContext ) -> AsyncGenerator [ coordinator_pb2 . Subscribe . REP , None ]: \"\"\" handle subscribe gRPC request, response job proposals in stream Args: request: context: Returns: \"\"\" if request . party_id in self . _enrolled : yield coordinator_pb2 . Subscribe . REP ( status = coordinator_pb2 . Subscribe . DUPLICATE_ENROLL ) return if not self . _serving : yield coordinator_pb2 . Subscribe . REP ( status = coordinator_pb2 . Subscribe . NOT_SERVING ) return task_provider = _TaskProviderForEnrolledParty () self . _enrolled [ request . party_id ] = task_provider for job_type in request . job_types : self . _job_type_to_subscribes . setdefault ( job_type , set ()) . add ( request . party_id ) while True : if task_provider . closed : break try : # make stop subscribe passable, check status regularly proposal = await asyncio . wait_for ( task_provider . queue . get (), timeout = self . _check_interval ) except asyncio . TimeoutError : # not receive proposal task for a while, maybe: # 1. just no new proposal # 2. flag has changed to false and no new proposal will in-queue continue else : yield coordinator_pb2 . Subscribe . REP ( status = coordinator_pb2 . Subscribe . SUCCESS , proposal_id = proposal . uid , job_type = proposal . job_type , ) task_provider . queue . task_done () # clean proposals while not task_provider . queue . empty (): await task_provider . queue . get () task_provider . queue . task_done ()","title":"coordinator"},{"location":"apis/coordinator/#coordinator","text":"","title":"Coordinator"},{"location":"apis/coordinator/#fedvision.framework.coordinator","text":"","title":"fedvision.framework.coordinator"},{"location":"apis/coordinator/#fedvision.framework.coordinator.coordinator","text":"","title":"coordinator"},{"location":"apis/coordinator/#fedvision.framework.coordinator.coordinator.Coordinator","text":"","title":"Coordinator"},{"location":"apis/coordinator/#fedvision.framework.coordinator.coordinator.Coordinator.__init__","text":"init coordinator Parameters: Name Type Description Default port int coordinator serving port required Source code in fedvision/framework/coordinator/coordinator.py def __init__ ( self , port : int ): \"\"\" init coordinator Args: port: coordinator serving port \"\"\" self . _serving = True self . _enrolled : MutableMapping [ str , _TaskProviderForEnrolledParty ] = {} self . _proposals : MutableMapping [ str , _Proposal ] = {} self . _job_type_to_subscribes : MutableMapping [ str , MutableSet [ str ]] = {} self . _check_interval = 0.5 self . _count_id = 0 self . _grpc_port = port self . _grpc_server = None","title":"__init__()"},{"location":"apis/coordinator/#fedvision.framework.coordinator.coordinator.Coordinator.FetchTask","text":"handle task fetch gRPC request Parameters: Name Type Description Default request coordinator_pb2.FetchTask.REQ required context grpc.aio.ServicerContext required Returns: Type Description coordinator_pb2.Proposal.REP Source code in fedvision/framework/coordinator/coordinator.py async def FetchTask ( self , request : coordinator_pb2 . FetchTask . REQ , context : grpc . aio . ServicerContext ) -> coordinator_pb2 . Proposal . REP : \"\"\" handle task fetch gRPC request Args: request: context: Returns: \"\"\" if request . proposal_id not in self . _proposals : return coordinator_pb2 . FetchTask . REP ( status = coordinator_pb2 . FetchTask . NOT_FOUND ) if request . party_id not in self . _enrolled : return coordinator_pb2 . FetchTask . REP ( status = coordinator_pb2 . FetchTask . NOT_ALLOW ) proposal = self . _proposals [ request . proposal_id ] if proposal . open_period_finished . is_set (): return coordinator_pb2 . FetchTask . REP ( status = coordinator_pb2 . FetchTask . TIMEOUT ) proposal . add_responders ( request . party_id ) await proposal . open_period_finished . wait () if not proposal . goal_reached : return coordinator_pb2 . FetchTask . REP ( status = coordinator_pb2 . FetchTask . CANCELED ) self . info ( f \"chosen: { proposal . chosen . keys () } \" ) if request . party_id not in proposal . chosen : return coordinator_pb2 . FetchTask . REP ( status = coordinator_pb2 . FetchTask . RANDOM_OUT ) # accepted, finally! success_rep = coordinator_pb2 . FetchTask . REP ( status = coordinator_pb2 . FetchTask . READY , task = proposal . chosen [ request . party_id ], ) return success_rep","title":"FetchTask()"},{"location":"apis/coordinator/#fedvision.framework.coordinator.coordinator.Coordinator.Leave","text":"Missing associated documentation comment in .proto file. Source code in fedvision/framework/coordinator/coordinator.py async def Leave ( self , request , context ): if request . party_id not in self . _enrolled : return coordinator_pb2 . Leave . REP ( status = coordinator_pb2 . Leave . NOT_FOUND ) self . _enrolled [ request . party_id ] . closed = True await self . _enrolled [ request . party_id ] . queue . join () self . _enrolled . __delitem__ ( request . party_id ) return coordinator_pb2 . Leave . REP ( status = coordinator_pb2 . Leave . SUCCESS )","title":"Leave()"},{"location":"apis/coordinator/#fedvision.framework.coordinator.coordinator.Coordinator.Proposal","text":"handle job proposal gRPC request Parameters: Name Type Description Default request coordinator_pb2.Proposal.REQ required context grpc.aio.ServicerContext required Returns: Type Description coordinator_pb2.Proposal.REP Source code in fedvision/framework/coordinator/coordinator.py async def Proposal ( self , request : coordinator_pb2 . Proposal . REQ , context : grpc . aio . ServicerContext ) -> coordinator_pb2 . Proposal . REP : \"\"\" handle job proposal gRPC request Args: request: context: Returns: \"\"\" uid = self . _generate_proposal_id ( request . job_id ) proposal = _Proposal . from_pb ( uid = uid , pb = request ) self . _proposals [ uid ] = proposal if proposal . job_type not in self . _job_type_to_subscribes : self . info ( f \"job type { proposal . job_type } not in { self . _job_type_to_subscribes } , reject\" ) return coordinator_pb2 . Proposal . REP ( status = coordinator_pb2 . Proposal . REJECT ) if ( len ( self . _job_type_to_subscribes [ proposal . job_type ]) < proposal . minimum_acceptance ): self . info ( f \"not enough parties alive accept job type { proposal . job_type } ,\" f \" required { proposal . minimum_acceptance } , \" f \" { len ( self . _job_type_to_subscribes [ proposal . job_type ]) } alive\" ) return coordinator_pb2 . Proposal . REP ( status = coordinator_pb2 . Proposal . NOT_ENOUGH_SUBSCRIBERS ) # dispatch proposal for party_id in self . _job_type_to_subscribes [ proposal . job_type ]: await self . _enrolled [ party_id ] . queue . put ( proposal ) # sleep until timeout and then check if there are enough responders await asyncio . sleep ( request . proposal_wait_time ) if not proposal . has_enough_responders (): proposal . set_open_period_finished ( goal_reached = False ) return coordinator_pb2 . Proposal . REP ( status = coordinator_pb2 . Proposal . NOT_ENOUGH_RESPONDERS ) proposal . set_open_period_finished ( goal_reached = True ) return coordinator_pb2 . Proposal . REP ( status = coordinator_pb2 . Proposal . SUCCESS )","title":"Proposal()"},{"location":"apis/coordinator/#fedvision.framework.coordinator.coordinator.Coordinator.Subscribe","text":"handle subscribe gRPC request, response job proposals in stream Parameters: Name Type Description Default request coordinator_pb2.Subscribe.REQ required context grpc.aio.ServicerContext required Returns: Type Description AsyncGenerator[coordinator_pb2.Subscribe.REP, None] Source code in fedvision/framework/coordinator/coordinator.py async def Subscribe ( self , request : coordinator_pb2 . Subscribe . REQ , context : grpc . aio . ServicerContext ) -> AsyncGenerator [ coordinator_pb2 . Subscribe . REP , None ]: \"\"\" handle subscribe gRPC request, response job proposals in stream Args: request: context: Returns: \"\"\" if request . party_id in self . _enrolled : yield coordinator_pb2 . Subscribe . REP ( status = coordinator_pb2 . Subscribe . DUPLICATE_ENROLL ) return if not self . _serving : yield coordinator_pb2 . Subscribe . REP ( status = coordinator_pb2 . Subscribe . NOT_SERVING ) return task_provider = _TaskProviderForEnrolledParty () self . _enrolled [ request . party_id ] = task_provider for job_type in request . job_types : self . _job_type_to_subscribes . setdefault ( job_type , set ()) . add ( request . party_id ) while True : if task_provider . closed : break try : # make stop subscribe passable, check status regularly proposal = await asyncio . wait_for ( task_provider . queue . get (), timeout = self . _check_interval ) except asyncio . TimeoutError : # not receive proposal task for a while, maybe: # 1. just no new proposal # 2. flag has changed to false and no new proposal will in-queue continue else : yield coordinator_pb2 . Subscribe . REP ( status = coordinator_pb2 . Subscribe . SUCCESS , proposal_id = proposal . uid , job_type = proposal . job_type , ) task_provider . queue . task_done () # clean proposals while not task_provider . queue . empty (): await task_provider . queue . get () task_provider . queue . task_done ()","title":"Subscribe()"},{"location":"apis/master/","text":"Coordinator \u00b6 \u00b6 ClusterManagerConnect \u00b6 cluster manager client __init__ ( self , address , shared_status ) special \u00b6 init cluster manager client Parameters: Name Type Description Default address required shared_status _SharedStatus required Source code in fedvision/framework/master/master.py def __init__ ( self , address , shared_status : _SharedStatus ): \"\"\" init cluster manager client Args: address: shared_status: \"\"\" self . address = address self . shared_status = shared_status self . _channel : Optional [ grpc . aio . Channel ] = None self . _stub : Optional [ cluster_pb2_grpc . ClusterManagerStub ] = None cluster_channel_ready ( self ) async \u00b6 await until channel ready Source code in fedvision/framework/master/master.py async def cluster_channel_ready ( self ): \"\"\" await until channel ready \"\"\" return await self . _channel . channel_ready () start_cluster_channel ( self ) async \u00b6 start channel to cluster manager Source code in fedvision/framework/master/master.py async def start_cluster_channel ( self ): \"\"\" start channel to cluster manager \"\"\" self . info ( f \"start cluster channel to { self . address } \" ) self . _channel = grpc . aio . insecure_channel ( self . address , options = [ ( \"grpc.max_send_message_length\" , 512 * 1024 * 1024 ), ( \"grpc.max_receive_message_length\" , 512 * 1024 * 1024 ), ], ) self . _stub = cluster_pb2_grpc . ClusterManagerStub ( self . _channel ) self . info ( f \"cluster channel started to { self . address } \" ) stop_cluster_channel ( self , grace = None ) async \u00b6 stop channel to cluster manager Parameters: Name Type Description Default grace Optional[float] None Source code in fedvision/framework/master/master.py async def stop_cluster_channel ( self , grace : Optional [ float ] = None ): \"\"\" stop channel to cluster manager Args: grace: Returns: \"\"\" self . info ( f \"stopping cluster channel\" ) await self . _channel . close ( grace ) self . info ( f \"cluster channel started to { self . address } \" ) submit_tasks_to_cluster ( self ) async \u00b6 infinity loop to get task from queue and submit it to cluster Source code in fedvision/framework/master/master.py async def submit_tasks_to_cluster ( self ): \"\"\" infinity loop to get task from queue and submit it to cluster \"\"\" while True : task = await self . shared_status . cluster_task_queue . get () self . debug ( f \"task sending: task_id= { task . task_id } task_type= { task . task_type } to cluster\" ) await self . _stub . TaskSubmit ( cluster_pb2 . TaskSubmit . REQ ( task = task )) self . debug ( f \"task sent: task_id= { task . task_id } task_type= { task . task_type } to cluster\" ) task_resource_require ( self , request ) async \u00b6 acquired resource from cluster(ports) Parameters: Name Type Description Default request cluster_pb2.TaskResourceRequire.REQ required Returns: Type Description cluster_pb2.TaskResourceRequire.REP Source code in fedvision/framework/master/master.py async def task_resource_require ( self , request : cluster_pb2 . TaskResourceRequire . REQ ) -> cluster_pb2 . TaskResourceRequire . REP : \"\"\" acquired resource from cluster(ports) Args: request: Returns: \"\"\" response = await self . _stub . TaskResourceRequire ( request ) return response CoordinatorConnect \u00b6 client connects to coordinator __init__ ( self , address , shared_status ) special \u00b6 init coordinator client Parameters: Name Type Description Default address str str required shared_status _SharedStatus required Source code in fedvision/framework/master/master.py def __init__ ( self , address : str , shared_status : _SharedStatus ): \"\"\" init coordinator client Args: address: str shared_status: \"\"\" self . address = address self . shared_status = shared_status self . accept_rule = ProposalAcceptRule ( self . shared_status ) self . _channel = None self . _stub = None coordinator_channel_ready ( self ) async \u00b6 wait until channel ready Source code in fedvision/framework/master/master.py async def coordinator_channel_ready ( self ): \"\"\" wait until channel ready \"\"\" return await self . _channel . channel_ready () leave ( self ) async \u00b6 disconnect with coordinator Source code in fedvision/framework/master/master.py async def leave ( self ): \"\"\" disconnect with coordinator \"\"\" return await self . _stub . Leave ( coordinator_pb2 . Leave . REQ ( party_id = self . shared_status . party_id ) ) make_proposal ( self , request ) async \u00b6 publish a job proposal to coordinator Parameters: Name Type Description Default request coordinator_pb2.Proposal.REQ request required Returns: Type Description coordinator_pb2.Proposal.REP response Source code in fedvision/framework/master/master.py async def make_proposal ( self , request : coordinator_pb2 . Proposal . REQ ) -> coordinator_pb2 . Proposal . REP : \"\"\" publish a job proposal to coordinator Args: request: request Returns: response \"\"\" return await self . _stub . Proposal ( request ) start_coordinator_channel ( self ) async \u00b6 start channel to coordinator Source code in fedvision/framework/master/master.py async def start_coordinator_channel ( self ): \"\"\" start channel to coordinator \"\"\" self . info ( f \"start coordinator channel to { self . address } \" ) self . _channel = grpc . aio . insecure_channel ( self . address , options = [ ( \"grpc.max_send_message_length\" , 512 * 1024 * 1024 ), ( \"grpc.max_receive_message_length\" , 512 * 1024 * 1024 ), ], ) self . _stub = coordinator_pb2_grpc . CoordinatorStub ( self . _channel , ) self . info ( f \"coordinator channel started to { self . address } \" ) stop_coordinator_channel ( self , grace = None ) async \u00b6 stop channel Parameters: Name Type Description Default grace Optional[float] wait seconds to gracefully stop None Source code in fedvision/framework/master/master.py async def stop_coordinator_channel ( self , grace : Optional [ float ] = None ): \"\"\" stop channel Args: grace: wait seconds to gracefully stop \"\"\" self . info ( f \"stopping coordinator channel\" ) await self . _channel . close ( grace ) self . info ( f \"coordinator channel started to { self . address } \" ) subscribe ( self ) async \u00b6 start subscribe to coordinator and accept proposals Source code in fedvision/framework/master/master.py async def subscribe ( self ): \"\"\" start subscribe to coordinator and accept `proposals` \"\"\" request = coordinator_pb2 . Subscribe . REQ ( party_id = self . shared_status . party_id ) for job_type in self . shared_status . job_types : request . job_types . append ( job_type ) async for response in self . _stub . Subscribe ( request ): if response . status != coordinator_pb2 . Subscribe . SUCCESS : return if not await self . accept_rule . accept ( response . job_type ): return async def _acceptor (): # accept all fetch_response = await self . _stub . FetchTask ( coordinator_pb2 . FetchTask . REQ ( party_id = self . shared_status . party_id , proposal_id = response . proposal_id , ) ) if fetch_response . status != coordinator_pb2 . FetchTask . READY : self . debug ( f \"proposal { response . proposal_id } not ready: { fetch_response . status } \" ) return # put task in cluster task queue await self . shared_status . cluster_task_queue . put ( fetch_response . task ) asyncio . create_task ( _acceptor ()) Master \u00b6 __init__ ( self , party_id , coordinator_address , cluster_address , rest_port , rest_host = None ) special \u00b6 init master Parameters: Name Type Description Default party_id str required coordinator_address str required rest_port int required rest_host str None Source code in fedvision/framework/master/master.py def __init__ ( self , party_id : str , coordinator_address : str , cluster_address : str , rest_port : int , rest_host : str = None , ): \"\"\" init master Args: party_id: coordinator_address: rest_port: rest_host: \"\"\" self . shared_status = _SharedStatus ( party_id = party_id ) self . _coordinator = CoordinatorConnect ( shared_status = self . shared_status , address = coordinator_address ) self . _rest_site = RESTService ( shared_status = self . shared_status , port = rest_port , host = rest_host ) self . _cluster = ClusterManagerConnect ( shared_status = self . shared_status , address = cluster_address ) start ( self ) async \u00b6 start master: 1. cluster manager to process tasks 2. restful service to handler request from user 3. coordinator to connect to `the world` Source code in fedvision/framework/master/master.py async def start ( self ): \"\"\" start master: 1. cluster manager to process tasks 2. restful service to handler request from user 3. coordinator to connect to `the world` \"\"\" # connect to cluster await self . _cluster . start_cluster_channel () while True : try : await asyncio . wait_for ( self . _cluster . cluster_channel_ready (), 5 ) except asyncio . TimeoutError : self . warning ( f \"cluster channel not ready, retry in 5 seconds\" ) else : self . info ( f \"cluster channel ready!\" ) break asyncio . create_task ( self . _cluster . submit_tasks_to_cluster ()) # start rest site await self . _rest_site . start_rest_site () # connect to coordinator await self . _coordinator . start_coordinator_channel () while True : try : await asyncio . wait_for ( self . _coordinator . coordinator_channel_ready (), 5 ) except asyncio . TimeoutError : self . warning ( f \"coordinator channel not ready, retry in 5 seconds\" ) else : self . info ( f \"coordinator channel ready!\" ) break asyncio . create_task ( self . _coordinator . subscribe ()) # job process loop: # 1. get job from rest site # 2. make proposal to coordinator # 3. send task to cluster by put it into a queue asyncio . create_task ( self . _submitted_job_handler ()) stop ( self ) async \u00b6 stop master Source code in fedvision/framework/master/master.py async def stop ( self ): \"\"\" stop master \"\"\" await self . _coordinator . stop_coordinator_channel ( grace = 1 ) await self . _rest_site . stop_rest_site () await self . _cluster . stop_cluster_channel ( grace = 1 ) ProposalAcceptRule \u00b6 __eq__ ( self , other ) special \u00b6 Method generated by attrs for class ProposalAcceptRule. Source code in fedvision/framework/master/master.py def __eq__ ( self , other ): if other . __class__ is not self . __class__ : return NotImplemented return ( self . shared_status , ) == ( other . shared_status , ) __ge__ ( self , other ) special \u00b6 Method generated by attrs for class ProposalAcceptRule. Source code in fedvision/framework/master/master.py def __ge__ ( self , other ): \"\"\" Automatically created by attrs. \"\"\" if other . __class__ is self . __class__ : return attrs_to_tuple ( self ) >= attrs_to_tuple ( other ) return NotImplemented __gt__ ( self , other ) special \u00b6 Method generated by attrs for class ProposalAcceptRule. Source code in fedvision/framework/master/master.py def __gt__ ( self , other ): \"\"\" Automatically created by attrs. \"\"\" if other . __class__ is self . __class__ : return attrs_to_tuple ( self ) > attrs_to_tuple ( other ) return NotImplemented __init__ ( self , shared_status ) special \u00b6 Method generated by attrs for class ProposalAcceptRule. Source code in fedvision/framework/master/master.py def __init__ ( self , shared_status ): self . shared_status = shared_status __le__ ( self , other ) special \u00b6 Method generated by attrs for class ProposalAcceptRule. Source code in fedvision/framework/master/master.py def __le__ ( self , other ): \"\"\" Automatically created by attrs. \"\"\" if other . __class__ is self . __class__ : return attrs_to_tuple ( self ) <= attrs_to_tuple ( other ) return NotImplemented __lt__ ( self , other ) special \u00b6 Method generated by attrs for class ProposalAcceptRule. Source code in fedvision/framework/master/master.py def __lt__ ( self , other ): \"\"\" Automatically created by attrs. \"\"\" if other . __class__ is self . __class__ : return attrs_to_tuple ( self ) < attrs_to_tuple ( other ) return NotImplemented __ne__ ( self , other ) special \u00b6 Method generated by attrs for class ProposalAcceptRule. Source code in fedvision/framework/master/master.py def __ne__ ( self , other ): \"\"\" Check equality and either forward a NotImplemented or return the result negated. \"\"\" result = self . __eq__ ( other ) if result is NotImplemented : return NotImplemented return not result __repr__ ( self ) special \u00b6 Method generated by attrs for class ProposalAcceptRule. Source code in fedvision/framework/master/master.py def __repr__ ( self ): \"\"\" Automatically created by attrs. \"\"\" try : working_set = _already_repring . working_set except AttributeError : working_set = set () _already_repring . working_set = working_set if id ( self ) in working_set : return \"...\" real_cls = self . __class__ if ns is None : qualname = getattr ( real_cls , \"__qualname__\" , None ) if qualname is not None : class_name = qualname . rsplit ( \">.\" , 1 )[ - 1 ] else : class_name = real_cls . __name__ else : class_name = ns + \".\" + real_cls . __name__ # Since 'self' remains on the stack (i.e.: strongly referenced) for the # duration of this call, it's safe to depend on id(...) stability, and # not need to track the instance and therefore worry about properties # like weakref- or hash-ability. working_set . add ( id ( self )) try : result = [ class_name , \"(\" ] first = True for name , attr_repr in attr_names_with_reprs : if first : first = False else : result . append ( \", \" ) result . extend ( ( name , \"=\" , attr_repr ( getattr ( self , name , NOTHING ))) ) return \"\" . join ( result ) + \")\" finally : working_set . remove ( id ( self )) RESTService \u00b6 service accept restful request from users __init__ ( self , shared_status , port , host = None ) special \u00b6 init rest services instance Parameters: Name Type Description Default shared_status _SharedStatus required port int required host str None Source code in fedvision/framework/master/master.py def __init__ ( self , shared_status : _SharedStatus , port : int , host : str = None ): \"\"\" init rest services instance Args: shared_status: port: host: \"\"\" self . shared_status = shared_status self . port = port self . host = host self . _site : Optional [ web . TCPSite ] = None start_rest_site ( self ) async \u00b6 start web service non-blocked Source code in fedvision/framework/master/master.py async def start_rest_site ( self ): \"\"\" start web service non-blocked \"\"\" self . info ( f \"starting restful services at { ':' if self . host is None else self . host } : { self . port } \" ) app = web . Application () app . add_routes ( self . _register_routes ()) runner = web . AppRunner ( app , access_log = self . get_logger ()) await runner . setup () self . _site = web . TCPSite ( runner = runner , host = self . host , port = self . port ) await self . _site . start () self . info ( f \"restful services started at { ':' if self . host is None else self . host } : { self . port } \" ) stop_rest_site ( self ) async \u00b6 stop web service Source code in fedvision/framework/master/master.py async def stop_rest_site ( self ): \"\"\" stop web service \"\"\" if self . _site is not None : await self . _site . stop ()","title":"master"},{"location":"apis/master/#coordinator","text":"","title":"Coordinator"},{"location":"apis/master/#fedvision.framework.master.master","text":"","title":"fedvision.framework.master.master"},{"location":"apis/master/#fedvision.framework.master.master.ClusterManagerConnect","text":"cluster manager client","title":"ClusterManagerConnect"},{"location":"apis/master/#fedvision.framework.master.master.ClusterManagerConnect.__init__","text":"init cluster manager client Parameters: Name Type Description Default address required shared_status _SharedStatus required Source code in fedvision/framework/master/master.py def __init__ ( self , address , shared_status : _SharedStatus ): \"\"\" init cluster manager client Args: address: shared_status: \"\"\" self . address = address self . shared_status = shared_status self . _channel : Optional [ grpc . aio . Channel ] = None self . _stub : Optional [ cluster_pb2_grpc . ClusterManagerStub ] = None","title":"__init__()"},{"location":"apis/master/#fedvision.framework.master.master.ClusterManagerConnect.cluster_channel_ready","text":"await until channel ready Source code in fedvision/framework/master/master.py async def cluster_channel_ready ( self ): \"\"\" await until channel ready \"\"\" return await self . _channel . channel_ready ()","title":"cluster_channel_ready()"},{"location":"apis/master/#fedvision.framework.master.master.ClusterManagerConnect.start_cluster_channel","text":"start channel to cluster manager Source code in fedvision/framework/master/master.py async def start_cluster_channel ( self ): \"\"\" start channel to cluster manager \"\"\" self . info ( f \"start cluster channel to { self . address } \" ) self . _channel = grpc . aio . insecure_channel ( self . address , options = [ ( \"grpc.max_send_message_length\" , 512 * 1024 * 1024 ), ( \"grpc.max_receive_message_length\" , 512 * 1024 * 1024 ), ], ) self . _stub = cluster_pb2_grpc . ClusterManagerStub ( self . _channel ) self . info ( f \"cluster channel started to { self . address } \" )","title":"start_cluster_channel()"},{"location":"apis/master/#fedvision.framework.master.master.ClusterManagerConnect.stop_cluster_channel","text":"stop channel to cluster manager Parameters: Name Type Description Default grace Optional[float] None Source code in fedvision/framework/master/master.py async def stop_cluster_channel ( self , grace : Optional [ float ] = None ): \"\"\" stop channel to cluster manager Args: grace: Returns: \"\"\" self . info ( f \"stopping cluster channel\" ) await self . _channel . close ( grace ) self . info ( f \"cluster channel started to { self . address } \" )","title":"stop_cluster_channel()"},{"location":"apis/master/#fedvision.framework.master.master.ClusterManagerConnect.submit_tasks_to_cluster","text":"infinity loop to get task from queue and submit it to cluster Source code in fedvision/framework/master/master.py async def submit_tasks_to_cluster ( self ): \"\"\" infinity loop to get task from queue and submit it to cluster \"\"\" while True : task = await self . shared_status . cluster_task_queue . get () self . debug ( f \"task sending: task_id= { task . task_id } task_type= { task . task_type } to cluster\" ) await self . _stub . TaskSubmit ( cluster_pb2 . TaskSubmit . REQ ( task = task )) self . debug ( f \"task sent: task_id= { task . task_id } task_type= { task . task_type } to cluster\" )","title":"submit_tasks_to_cluster()"},{"location":"apis/master/#fedvision.framework.master.master.ClusterManagerConnect.task_resource_require","text":"acquired resource from cluster(ports) Parameters: Name Type Description Default request cluster_pb2.TaskResourceRequire.REQ required Returns: Type Description cluster_pb2.TaskResourceRequire.REP Source code in fedvision/framework/master/master.py async def task_resource_require ( self , request : cluster_pb2 . TaskResourceRequire . REQ ) -> cluster_pb2 . TaskResourceRequire . REP : \"\"\" acquired resource from cluster(ports) Args: request: Returns: \"\"\" response = await self . _stub . TaskResourceRequire ( request ) return response","title":"task_resource_require()"},{"location":"apis/master/#fedvision.framework.master.master.CoordinatorConnect","text":"client connects to coordinator","title":"CoordinatorConnect"},{"location":"apis/master/#fedvision.framework.master.master.CoordinatorConnect.__init__","text":"init coordinator client Parameters: Name Type Description Default address str str required shared_status _SharedStatus required Source code in fedvision/framework/master/master.py def __init__ ( self , address : str , shared_status : _SharedStatus ): \"\"\" init coordinator client Args: address: str shared_status: \"\"\" self . address = address self . shared_status = shared_status self . accept_rule = ProposalAcceptRule ( self . shared_status ) self . _channel = None self . _stub = None","title":"__init__()"},{"location":"apis/master/#fedvision.framework.master.master.CoordinatorConnect.coordinator_channel_ready","text":"wait until channel ready Source code in fedvision/framework/master/master.py async def coordinator_channel_ready ( self ): \"\"\" wait until channel ready \"\"\" return await self . _channel . channel_ready ()","title":"coordinator_channel_ready()"},{"location":"apis/master/#fedvision.framework.master.master.CoordinatorConnect.leave","text":"disconnect with coordinator Source code in fedvision/framework/master/master.py async def leave ( self ): \"\"\" disconnect with coordinator \"\"\" return await self . _stub . Leave ( coordinator_pb2 . Leave . REQ ( party_id = self . shared_status . party_id ) )","title":"leave()"},{"location":"apis/master/#fedvision.framework.master.master.CoordinatorConnect.make_proposal","text":"publish a job proposal to coordinator Parameters: Name Type Description Default request coordinator_pb2.Proposal.REQ request required Returns: Type Description coordinator_pb2.Proposal.REP response Source code in fedvision/framework/master/master.py async def make_proposal ( self , request : coordinator_pb2 . Proposal . REQ ) -> coordinator_pb2 . Proposal . REP : \"\"\" publish a job proposal to coordinator Args: request: request Returns: response \"\"\" return await self . _stub . Proposal ( request )","title":"make_proposal()"},{"location":"apis/master/#fedvision.framework.master.master.CoordinatorConnect.start_coordinator_channel","text":"start channel to coordinator Source code in fedvision/framework/master/master.py async def start_coordinator_channel ( self ): \"\"\" start channel to coordinator \"\"\" self . info ( f \"start coordinator channel to { self . address } \" ) self . _channel = grpc . aio . insecure_channel ( self . address , options = [ ( \"grpc.max_send_message_length\" , 512 * 1024 * 1024 ), ( \"grpc.max_receive_message_length\" , 512 * 1024 * 1024 ), ], ) self . _stub = coordinator_pb2_grpc . CoordinatorStub ( self . _channel , ) self . info ( f \"coordinator channel started to { self . address } \" )","title":"start_coordinator_channel()"},{"location":"apis/master/#fedvision.framework.master.master.CoordinatorConnect.stop_coordinator_channel","text":"stop channel Parameters: Name Type Description Default grace Optional[float] wait seconds to gracefully stop None Source code in fedvision/framework/master/master.py async def stop_coordinator_channel ( self , grace : Optional [ float ] = None ): \"\"\" stop channel Args: grace: wait seconds to gracefully stop \"\"\" self . info ( f \"stopping coordinator channel\" ) await self . _channel . close ( grace ) self . info ( f \"coordinator channel started to { self . address } \" )","title":"stop_coordinator_channel()"},{"location":"apis/master/#fedvision.framework.master.master.CoordinatorConnect.subscribe","text":"start subscribe to coordinator and accept proposals Source code in fedvision/framework/master/master.py async def subscribe ( self ): \"\"\" start subscribe to coordinator and accept `proposals` \"\"\" request = coordinator_pb2 . Subscribe . REQ ( party_id = self . shared_status . party_id ) for job_type in self . shared_status . job_types : request . job_types . append ( job_type ) async for response in self . _stub . Subscribe ( request ): if response . status != coordinator_pb2 . Subscribe . SUCCESS : return if not await self . accept_rule . accept ( response . job_type ): return async def _acceptor (): # accept all fetch_response = await self . _stub . FetchTask ( coordinator_pb2 . FetchTask . REQ ( party_id = self . shared_status . party_id , proposal_id = response . proposal_id , ) ) if fetch_response . status != coordinator_pb2 . FetchTask . READY : self . debug ( f \"proposal { response . proposal_id } not ready: { fetch_response . status } \" ) return # put task in cluster task queue await self . shared_status . cluster_task_queue . put ( fetch_response . task ) asyncio . create_task ( _acceptor ())","title":"subscribe()"},{"location":"apis/master/#fedvision.framework.master.master.Master","text":"","title":"Master"},{"location":"apis/master/#fedvision.framework.master.master.Master.__init__","text":"init master Parameters: Name Type Description Default party_id str required coordinator_address str required rest_port int required rest_host str None Source code in fedvision/framework/master/master.py def __init__ ( self , party_id : str , coordinator_address : str , cluster_address : str , rest_port : int , rest_host : str = None , ): \"\"\" init master Args: party_id: coordinator_address: rest_port: rest_host: \"\"\" self . shared_status = _SharedStatus ( party_id = party_id ) self . _coordinator = CoordinatorConnect ( shared_status = self . shared_status , address = coordinator_address ) self . _rest_site = RESTService ( shared_status = self . shared_status , port = rest_port , host = rest_host ) self . _cluster = ClusterManagerConnect ( shared_status = self . shared_status , address = cluster_address )","title":"__init__()"},{"location":"apis/master/#fedvision.framework.master.master.Master.start","text":"start master: 1. cluster manager to process tasks 2. restful service to handler request from user 3. coordinator to connect to `the world` Source code in fedvision/framework/master/master.py async def start ( self ): \"\"\" start master: 1. cluster manager to process tasks 2. restful service to handler request from user 3. coordinator to connect to `the world` \"\"\" # connect to cluster await self . _cluster . start_cluster_channel () while True : try : await asyncio . wait_for ( self . _cluster . cluster_channel_ready (), 5 ) except asyncio . TimeoutError : self . warning ( f \"cluster channel not ready, retry in 5 seconds\" ) else : self . info ( f \"cluster channel ready!\" ) break asyncio . create_task ( self . _cluster . submit_tasks_to_cluster ()) # start rest site await self . _rest_site . start_rest_site () # connect to coordinator await self . _coordinator . start_coordinator_channel () while True : try : await asyncio . wait_for ( self . _coordinator . coordinator_channel_ready (), 5 ) except asyncio . TimeoutError : self . warning ( f \"coordinator channel not ready, retry in 5 seconds\" ) else : self . info ( f \"coordinator channel ready!\" ) break asyncio . create_task ( self . _coordinator . subscribe ()) # job process loop: # 1. get job from rest site # 2. make proposal to coordinator # 3. send task to cluster by put it into a queue asyncio . create_task ( self . _submitted_job_handler ())","title":"start()"},{"location":"apis/master/#fedvision.framework.master.master.Master.stop","text":"stop master Source code in fedvision/framework/master/master.py async def stop ( self ): \"\"\" stop master \"\"\" await self . _coordinator . stop_coordinator_channel ( grace = 1 ) await self . _rest_site . stop_rest_site () await self . _cluster . stop_cluster_channel ( grace = 1 )","title":"stop()"},{"location":"apis/master/#fedvision.framework.master.master.ProposalAcceptRule","text":"","title":"ProposalAcceptRule"},{"location":"apis/master/#fedvision.framework.master.master.ProposalAcceptRule.__eq__","text":"Method generated by attrs for class ProposalAcceptRule. Source code in fedvision/framework/master/master.py def __eq__ ( self , other ): if other . __class__ is not self . __class__ : return NotImplemented return ( self . shared_status , ) == ( other . shared_status , )","title":"__eq__()"},{"location":"apis/master/#fedvision.framework.master.master.ProposalAcceptRule.__ge__","text":"Method generated by attrs for class ProposalAcceptRule. Source code in fedvision/framework/master/master.py def __ge__ ( self , other ): \"\"\" Automatically created by attrs. \"\"\" if other . __class__ is self . __class__ : return attrs_to_tuple ( self ) >= attrs_to_tuple ( other ) return NotImplemented","title":"__ge__()"},{"location":"apis/master/#fedvision.framework.master.master.ProposalAcceptRule.__gt__","text":"Method generated by attrs for class ProposalAcceptRule. Source code in fedvision/framework/master/master.py def __gt__ ( self , other ): \"\"\" Automatically created by attrs. \"\"\" if other . __class__ is self . __class__ : return attrs_to_tuple ( self ) > attrs_to_tuple ( other ) return NotImplemented","title":"__gt__()"},{"location":"apis/master/#fedvision.framework.master.master.ProposalAcceptRule.__init__","text":"Method generated by attrs for class ProposalAcceptRule. Source code in fedvision/framework/master/master.py def __init__ ( self , shared_status ): self . shared_status = shared_status","title":"__init__()"},{"location":"apis/master/#fedvision.framework.master.master.ProposalAcceptRule.__le__","text":"Method generated by attrs for class ProposalAcceptRule. Source code in fedvision/framework/master/master.py def __le__ ( self , other ): \"\"\" Automatically created by attrs. \"\"\" if other . __class__ is self . __class__ : return attrs_to_tuple ( self ) <= attrs_to_tuple ( other ) return NotImplemented","title":"__le__()"},{"location":"apis/master/#fedvision.framework.master.master.ProposalAcceptRule.__lt__","text":"Method generated by attrs for class ProposalAcceptRule. Source code in fedvision/framework/master/master.py def __lt__ ( self , other ): \"\"\" Automatically created by attrs. \"\"\" if other . __class__ is self . __class__ : return attrs_to_tuple ( self ) < attrs_to_tuple ( other ) return NotImplemented","title":"__lt__()"},{"location":"apis/master/#fedvision.framework.master.master.ProposalAcceptRule.__ne__","text":"Method generated by attrs for class ProposalAcceptRule. Source code in fedvision/framework/master/master.py def __ne__ ( self , other ): \"\"\" Check equality and either forward a NotImplemented or return the result negated. \"\"\" result = self . __eq__ ( other ) if result is NotImplemented : return NotImplemented return not result","title":"__ne__()"},{"location":"apis/master/#fedvision.framework.master.master.ProposalAcceptRule.__repr__","text":"Method generated by attrs for class ProposalAcceptRule. Source code in fedvision/framework/master/master.py def __repr__ ( self ): \"\"\" Automatically created by attrs. \"\"\" try : working_set = _already_repring . working_set except AttributeError : working_set = set () _already_repring . working_set = working_set if id ( self ) in working_set : return \"...\" real_cls = self . __class__ if ns is None : qualname = getattr ( real_cls , \"__qualname__\" , None ) if qualname is not None : class_name = qualname . rsplit ( \">.\" , 1 )[ - 1 ] else : class_name = real_cls . __name__ else : class_name = ns + \".\" + real_cls . __name__ # Since 'self' remains on the stack (i.e.: strongly referenced) for the # duration of this call, it's safe to depend on id(...) stability, and # not need to track the instance and therefore worry about properties # like weakref- or hash-ability. working_set . add ( id ( self )) try : result = [ class_name , \"(\" ] first = True for name , attr_repr in attr_names_with_reprs : if first : first = False else : result . append ( \", \" ) result . extend ( ( name , \"=\" , attr_repr ( getattr ( self , name , NOTHING ))) ) return \"\" . join ( result ) + \")\" finally : working_set . remove ( id ( self ))","title":"__repr__()"},{"location":"apis/master/#fedvision.framework.master.master.RESTService","text":"service accept restful request from users","title":"RESTService"},{"location":"apis/master/#fedvision.framework.master.master.RESTService.__init__","text":"init rest services instance Parameters: Name Type Description Default shared_status _SharedStatus required port int required host str None Source code in fedvision/framework/master/master.py def __init__ ( self , shared_status : _SharedStatus , port : int , host : str = None ): \"\"\" init rest services instance Args: shared_status: port: host: \"\"\" self . shared_status = shared_status self . port = port self . host = host self . _site : Optional [ web . TCPSite ] = None","title":"__init__()"},{"location":"apis/master/#fedvision.framework.master.master.RESTService.start_rest_site","text":"start web service non-blocked Source code in fedvision/framework/master/master.py async def start_rest_site ( self ): \"\"\" start web service non-blocked \"\"\" self . info ( f \"starting restful services at { ':' if self . host is None else self . host } : { self . port } \" ) app = web . Application () app . add_routes ( self . _register_routes ()) runner = web . AppRunner ( app , access_log = self . get_logger ()) await runner . setup () self . _site = web . TCPSite ( runner = runner , host = self . host , port = self . port ) await self . _site . start () self . info ( f \"restful services started at { ':' if self . host is None else self . host } : { self . port } \" )","title":"start_rest_site()"},{"location":"apis/master/#fedvision.framework.master.master.RESTService.stop_rest_site","text":"stop web service Source code in fedvision/framework/master/master.py async def stop_rest_site ( self ): \"\"\" stop web service \"\"\" if self . _site is not None : await self . _site . stop ()","title":"stop_rest_site()"},{"location":"deploy/cli/","text":"CLI \u00b6 Usage : $ [ OPTIONS ] COMMAND [ ARGS ] ... Options : --install-completion : Install completion for the current shell. --show-completion : Show completion for the current shell, to copy it or customize the installation. --help : Show this message and exit. Commands : deploy : deploy tools services : services [start|stop] tools template : template tools deploy \u00b6 deploy tools Usage : $ deploy [ OPTIONS ] COMMAND [ ARGS ] ... Options : --help : Show this message and exit. Commands : deploy deploy deploy \u00b6 Usage : $ deploy deploy [ OPTIONS ] Options : --config FILE : [required] --help : Show this message and exit. services \u00b6 services [start|stop] tools Usage : $ services [ OPTIONS ] COMMAND [ ARGS ] ... Options : --help : Show this message and exit. Commands : all : [start|stop] all services cluster-manager : [start|stop] cluster manager service cluster-worker : [start|stop] cluster worker service coordinator : [start|stop] coordinator service master : [start|stop] master service services all \u00b6 [start|stop] all services Usage : $ services all [ OPTIONS ] COMMAND [ ARGS ] ... Options : --help : Show this message and exit. Commands : start : start all services stop : stop all services services all start \u00b6 start all services Usage : $ services all start [ OPTIONS ] CONFIG Arguments : CONFIG : [required] Options : --help : Show this message and exit. services all stop \u00b6 stop all services Usage : $ services all stop [ OPTIONS ] CONFIG Arguments : CONFIG : [required] Options : --help : Show this message and exit. services cluster-manager \u00b6 [start|stop] cluster manager service Usage : $ services cluster-manager [ OPTIONS ] COMMAND [ ARGS ] ... Options : --help : Show this message and exit. Commands : start : start cluster manager stop : stop cluster manager services cluster-manager start \u00b6 start cluster manager Usage : $ services cluster-manager start [ OPTIONS ] MACHINE_SSH MACHINE_BASE_DIR MANAGER_PORT Arguments : MACHINE_SSH : machine ssh string: user@host:port [required] MACHINE_BASE_DIR : deployed base name [required] MANAGER_PORT : port number for cluster manager to serve [required] Options : --help : Show this message and exit. services cluster-manager stop \u00b6 stop cluster manager Usage : $ services cluster-manager stop [ OPTIONS ] MACHINE_SSH MACHINE_BASE_DIR MANAGER_PORT Arguments : MACHINE_SSH : machine ssh string: user@host:port [required] MACHINE_BASE_DIR : deployed base name [required] MANAGER_PORT : port number for cluster manager to serve [required] Options : --help : Show this message and exit. services cluster-worker \u00b6 [start|stop] cluster worker service Usage : $ services cluster-worker [ OPTIONS ] COMMAND [ ARGS ] ... Options : --help : Show this message and exit. Commands : start : start cluster worker stop : stop cluster worker services cluster-worker start \u00b6 start cluster worker Usage : $ services cluster-worker start [ OPTIONS ] MACHINE_SSH MACHINE_BASE_DIR NAME LOCAL_IP PORT_START PORT_END MAX_TASKS CLUSTER_MANAGER_ADDRESS Arguments : MACHINE_SSH : machine ssh string: user@host:port [required] MACHINE_BASE_DIR : deployed base name [required] NAME : worker name [required] LOCAL_IP : local ip [required] PORT_START : port start [required] PORT_END : port start [required] MAX_TASKS : num of maximum parallel tasks [required] CLUSTER_MANAGER_ADDRESS : cluster manager address [required] Options : --data-dir TEXT : data dir --help : Show this message and exit. services cluster-worker stop \u00b6 stop cluster worker Usage : $ services cluster-worker stop [ OPTIONS ] MACHINE_SSH MACHINE_BASE_DIR NAME Arguments : MACHINE_SSH : machine ssh string: user@host:port [required] MACHINE_BASE_DIR : deployed base name [required] NAME : worker name [required] Options : --help : Show this message and exit. services coordinator \u00b6 [start|stop] coordinator service Usage : $ services coordinator [ OPTIONS ] COMMAND [ ARGS ] ... Options : --help : Show this message and exit. Commands : start : start coordinator stop : stop coordinator services coordinator start \u00b6 start coordinator Usage : $ services coordinator start [ OPTIONS ] MACHINE_SSH MACHINE_BASE_DIR COORDINATOR_PORT Arguments : MACHINE_SSH : machine ssh string: user@host:port [required] MACHINE_BASE_DIR : deployed base name [required] COORDINATOR_PORT : port number for coordinator to serve [required] Options : --help : Show this message and exit. services coordinator stop \u00b6 stop coordinator Usage : $ services coordinator stop [ OPTIONS ] MACHINE_SSH MACHINE_BASE_DIR COORDINATOR_PORT Arguments : MACHINE_SSH : machine ssh string: user@host:port [required] MACHINE_BASE_DIR : deployed base name [required] COORDINATOR_PORT : port number for coordinator to serve [required] Options : --help : Show this message and exit. services master \u00b6 [start|stop] master service Usage : $ services master [ OPTIONS ] COMMAND [ ARGS ] ... Options : --help : Show this message and exit. Commands : start : start master stop : stop master services master start \u00b6 start master Usage : $ services master start [ OPTIONS ] MACHINE_SSH MACHINE_BASE_DIR SUBMIT_PORT PARTY_ID CLUSTER_MANAGER_ADDRESS COORDINATOR_ADDRESS Arguments : MACHINE_SSH : machine ssh string: user@host:port [required] MACHINE_BASE_DIR : deployed base name [required] SUBMIT_PORT : submit port [required] PARTY_ID : party id [required] CLUSTER_MANAGER_ADDRESS : cluster manager address [required] COORDINATOR_ADDRESS : coordinator address [required] Options : --help : Show this message and exit. services master stop \u00b6 stop master Usage : $ services master stop [ OPTIONS ] MACHINE_SSH MACHINE_BASE_DIR SUBMIT_PORT Arguments : MACHINE_SSH : machine ssh string: user@host:port [required] MACHINE_BASE_DIR : deployed base name [required] SUBMIT_PORT : submit port [required] Options : --help : Show this message and exit. template \u00b6 template tools Usage : $ template [ OPTIONS ] COMMAND [ ARGS ] ... Options : --help : Show this message and exit. Commands : generate : generate template standalone : generate template for standalone deploy template generate \u00b6 generate template Usage : $ template generate [ OPTIONS ] Options : --help : Show this message and exit. template standalone \u00b6 generate template for standalone deploy Usage : $ template standalone [ OPTIONS ] Options : --help : Show this message and exit.","title":"fedvision-deploy-toolkit-cli"},{"location":"deploy/cli/#cli","text":"Usage : $ [ OPTIONS ] COMMAND [ ARGS ] ... Options : --install-completion : Install completion for the current shell. --show-completion : Show completion for the current shell, to copy it or customize the installation. --help : Show this message and exit. Commands : deploy : deploy tools services : services [start|stop] tools template : template tools","title":"CLI"},{"location":"deploy/cli/#deploy","text":"deploy tools Usage : $ deploy [ OPTIONS ] COMMAND [ ARGS ] ... Options : --help : Show this message and exit. Commands : deploy","title":"deploy"},{"location":"deploy/cli/#deploy-deploy","text":"Usage : $ deploy deploy [ OPTIONS ] Options : --config FILE : [required] --help : Show this message and exit.","title":"deploy deploy"},{"location":"deploy/cli/#services","text":"services [start|stop] tools Usage : $ services [ OPTIONS ] COMMAND [ ARGS ] ... Options : --help : Show this message and exit. Commands : all : [start|stop] all services cluster-manager : [start|stop] cluster manager service cluster-worker : [start|stop] cluster worker service coordinator : [start|stop] coordinator service master : [start|stop] master service","title":"services"},{"location":"deploy/cli/#services-all","text":"[start|stop] all services Usage : $ services all [ OPTIONS ] COMMAND [ ARGS ] ... Options : --help : Show this message and exit. Commands : start : start all services stop : stop all services","title":"services all"},{"location":"deploy/cli/#services-all-start","text":"start all services Usage : $ services all start [ OPTIONS ] CONFIG Arguments : CONFIG : [required] Options : --help : Show this message and exit.","title":"services all start"},{"location":"deploy/cli/#services-all-stop","text":"stop all services Usage : $ services all stop [ OPTIONS ] CONFIG Arguments : CONFIG : [required] Options : --help : Show this message and exit.","title":"services all stop"},{"location":"deploy/cli/#services-cluster-manager","text":"[start|stop] cluster manager service Usage : $ services cluster-manager [ OPTIONS ] COMMAND [ ARGS ] ... Options : --help : Show this message and exit. Commands : start : start cluster manager stop : stop cluster manager","title":"services cluster-manager"},{"location":"deploy/cli/#services-cluster-manager-start","text":"start cluster manager Usage : $ services cluster-manager start [ OPTIONS ] MACHINE_SSH MACHINE_BASE_DIR MANAGER_PORT Arguments : MACHINE_SSH : machine ssh string: user@host:port [required] MACHINE_BASE_DIR : deployed base name [required] MANAGER_PORT : port number for cluster manager to serve [required] Options : --help : Show this message and exit.","title":"services cluster-manager start"},{"location":"deploy/cli/#services-cluster-manager-stop","text":"stop cluster manager Usage : $ services cluster-manager stop [ OPTIONS ] MACHINE_SSH MACHINE_BASE_DIR MANAGER_PORT Arguments : MACHINE_SSH : machine ssh string: user@host:port [required] MACHINE_BASE_DIR : deployed base name [required] MANAGER_PORT : port number for cluster manager to serve [required] Options : --help : Show this message and exit.","title":"services cluster-manager stop"},{"location":"deploy/cli/#services-cluster-worker","text":"[start|stop] cluster worker service Usage : $ services cluster-worker [ OPTIONS ] COMMAND [ ARGS ] ... Options : --help : Show this message and exit. Commands : start : start cluster worker stop : stop cluster worker","title":"services cluster-worker"},{"location":"deploy/cli/#services-cluster-worker-start","text":"start cluster worker Usage : $ services cluster-worker start [ OPTIONS ] MACHINE_SSH MACHINE_BASE_DIR NAME LOCAL_IP PORT_START PORT_END MAX_TASKS CLUSTER_MANAGER_ADDRESS Arguments : MACHINE_SSH : machine ssh string: user@host:port [required] MACHINE_BASE_DIR : deployed base name [required] NAME : worker name [required] LOCAL_IP : local ip [required] PORT_START : port start [required] PORT_END : port start [required] MAX_TASKS : num of maximum parallel tasks [required] CLUSTER_MANAGER_ADDRESS : cluster manager address [required] Options : --data-dir TEXT : data dir --help : Show this message and exit.","title":"services cluster-worker start"},{"location":"deploy/cli/#services-cluster-worker-stop","text":"stop cluster worker Usage : $ services cluster-worker stop [ OPTIONS ] MACHINE_SSH MACHINE_BASE_DIR NAME Arguments : MACHINE_SSH : machine ssh string: user@host:port [required] MACHINE_BASE_DIR : deployed base name [required] NAME : worker name [required] Options : --help : Show this message and exit.","title":"services cluster-worker stop"},{"location":"deploy/cli/#services-coordinator","text":"[start|stop] coordinator service Usage : $ services coordinator [ OPTIONS ] COMMAND [ ARGS ] ... Options : --help : Show this message and exit. Commands : start : start coordinator stop : stop coordinator","title":"services coordinator"},{"location":"deploy/cli/#services-coordinator-start","text":"start coordinator Usage : $ services coordinator start [ OPTIONS ] MACHINE_SSH MACHINE_BASE_DIR COORDINATOR_PORT Arguments : MACHINE_SSH : machine ssh string: user@host:port [required] MACHINE_BASE_DIR : deployed base name [required] COORDINATOR_PORT : port number for coordinator to serve [required] Options : --help : Show this message and exit.","title":"services coordinator start"},{"location":"deploy/cli/#services-coordinator-stop","text":"stop coordinator Usage : $ services coordinator stop [ OPTIONS ] MACHINE_SSH MACHINE_BASE_DIR COORDINATOR_PORT Arguments : MACHINE_SSH : machine ssh string: user@host:port [required] MACHINE_BASE_DIR : deployed base name [required] COORDINATOR_PORT : port number for coordinator to serve [required] Options : --help : Show this message and exit.","title":"services coordinator stop"},{"location":"deploy/cli/#services-master","text":"[start|stop] master service Usage : $ services master [ OPTIONS ] COMMAND [ ARGS ] ... Options : --help : Show this message and exit. Commands : start : start master stop : stop master","title":"services master"},{"location":"deploy/cli/#services-master-start","text":"start master Usage : $ services master start [ OPTIONS ] MACHINE_SSH MACHINE_BASE_DIR SUBMIT_PORT PARTY_ID CLUSTER_MANAGER_ADDRESS COORDINATOR_ADDRESS Arguments : MACHINE_SSH : machine ssh string: user@host:port [required] MACHINE_BASE_DIR : deployed base name [required] SUBMIT_PORT : submit port [required] PARTY_ID : party id [required] CLUSTER_MANAGER_ADDRESS : cluster manager address [required] COORDINATOR_ADDRESS : coordinator address [required] Options : --help : Show this message and exit.","title":"services master start"},{"location":"deploy/cli/#services-master-stop","text":"stop master Usage : $ services master stop [ OPTIONS ] MACHINE_SSH MACHINE_BASE_DIR SUBMIT_PORT Arguments : MACHINE_SSH : machine ssh string: user@host:port [required] MACHINE_BASE_DIR : deployed base name [required] SUBMIT_PORT : submit port [required] Options : --help : Show this message and exit.","title":"services master stop"},{"location":"deploy/cli/#template","text":"template tools Usage : $ template [ OPTIONS ] COMMAND [ ARGS ] ... Options : --help : Show this message and exit. Commands : generate : generate template standalone : generate template for standalone deploy","title":"template"},{"location":"deploy/cli/#template-generate","text":"generate template Usage : $ template generate [ OPTIONS ] Options : --help : Show this message and exit.","title":"template generate"},{"location":"deploy/cli/#template-standalone","text":"generate template for standalone deploy Usage : $ template standalone [ OPTIONS ] Options : --help : Show this message and exit.","title":"template standalone"},{"location":"develop/codestyle/","text":"We use black and flake8 to format codes in fedvision . For developer, it's quiet easy to install pre-commit hooks: pip install pre-commit pre-commit install This will check code style of changed files before you push codes.","title":"codestyle"},{"location":"framework/overview/","text":"Overview \u00b6 There are two role associated in FedVision : coordinator Party Coordinator is an independent role responsible for handling job publish and distribute subtasks to proper parties that have subscribed to the coordinator . Usually, the coordinator must be started before any party can subscribe to it. The party will post the job of the specified job_type to the coordinator and wait for proposal_waiting_time seconds. While waiting, all party subscribed to this job_type will received a message, and then decide whether to participate or not. After the waiting time is over, the coordinator selects a group of party as a participants in this job. Party is an independent role that publishes or subscribe jobs. usually, It has a Cluster to process assigned tasks, and a submit service to process work requests from Users , a coordinator clients publish jobs to Coordinator or subscribe jobs from \"Coordinator\" and, a master` to mixes them up. Job's Life Cycle \u00b6 The FedVision framework is an extensible framework. When the master receives the job submit request, it will use different job loader according to the job_type parameter. This makes it possible to extent FedVision framework with various machine learning frameworks. Currently, only PaddlePaddle supported with extension configuration file: PaddleFL : jobs : - name : paddle_fl schema : ../schema/paddle_fl.json loader : fedvision.paddle_fl.job:PaddleFLJob tasks : - name : fl_trainer loader : fedvision.paddle_fl.tasks.task.trainer:FLTrainer - name : fl_aggregator loader : fedvision.paddle_fl.tasks.task.aggregator:FLAggregator To implement an new extension, one need to add configuration to extensions.yaml implement abstract job class implement several abstract task class used by job.","title":"framework"},{"location":"framework/overview/#overview","text":"There are two role associated in FedVision : coordinator Party Coordinator is an independent role responsible for handling job publish and distribute subtasks to proper parties that have subscribed to the coordinator . Usually, the coordinator must be started before any party can subscribe to it. The party will post the job of the specified job_type to the coordinator and wait for proposal_waiting_time seconds. While waiting, all party subscribed to this job_type will received a message, and then decide whether to participate or not. After the waiting time is over, the coordinator selects a group of party as a participants in this job. Party is an independent role that publishes or subscribe jobs. usually, It has a Cluster to process assigned tasks, and a submit service to process work requests from Users , a coordinator clients publish jobs to Coordinator or subscribe jobs from \"Coordinator\" and, a master` to mixes them up.","title":"Overview"},{"location":"framework/overview/#jobs-life-cycle","text":"The FedVision framework is an extensible framework. When the master receives the job submit request, it will use different job loader according to the job_type parameter. This makes it possible to extent FedVision framework with various machine learning frameworks. Currently, only PaddlePaddle supported with extension configuration file: PaddleFL : jobs : - name : paddle_fl schema : ../schema/paddle_fl.json loader : fedvision.paddle_fl.job:PaddleFLJob tasks : - name : fl_trainer loader : fedvision.paddle_fl.tasks.task.trainer:FLTrainer - name : fl_aggregator loader : fedvision.paddle_fl.tasks.task.aggregator:FLAggregator To implement an new extension, one need to add configuration to extensions.yaml implement abstract job class implement several abstract task class used by job.","title":"Job's Life Cycle"},{"location":"framework/paddledetection/","text":"Federated Job using PaddleDetection and PaddleFL are supported out of box. To submit a PaddleDetection jobs, one need a yaml config file to describe what algorithm and parameters to used. This is almost same as these paddle detection configs except that reference other config file are not supported. One can find an example using yolo mobilenet to detect fruit data in examples .","title":"paddledetection"},{"location":"framework/paddlefl/","text":"We provide a mnist demo to describes how to implement a job directly from PaddleFL . There are two py file related: fl_master : define CNN network to learning mnist dataset and compiled with PaddleFL fl_trainer : read dataset and training with compiled program. Just implement your demo by mimic this demo as you wish.","title":"paddlefl"},{"location":"quickstart/quickstart/","text":"This section describes how to quick deploy and run examples in standalone version of FedVision Prerequisites \u00b6 Too run Fedvision, following dependency or tools required: machine to install fedvision-deploy-toolkit: python virtualenv with Python>=3 setup SSH password less login to machine(s) for deploy fedvision framework. machine(s) to deploy fedvision framework: Python>=3.7(with pip) an isolated directory (each directory will be deployed with a copy of code) Deploy \u00b6 // create and activate python virtual environment $ python3 -V Python 3.9.0 $ python3 -m venv venv && source venv/bin/activate // install fedvision_deploy_toolkit $ ( venv ) python -m pip install -U pip && python -m pip install fedvision_deploy_toolkit ---> 100% Successfully installed fedvision_deploy_toolkit // generate deploy template $ ( venv ) fedvision-deploy template standalone // read comments in generated template standalone_template.yaml` and modify as you want. // deploy now $ ( venv ) fedvision-deploy deploy deploy standalone_template.yaml deploying 2 machines: ['machine1'] ---> 100% deploy done Note Deploying Cluster version is almost same except that you should generate template using fedvision-deploy template template and modify generated template file according to comments. Services start \u00b6 Services could be start/stop with scripts in Fedvision/sbin or, use fedvision deploy toolkits: // start services $ fedvision-deploy services all start standalone_template.yaml staring coordinator coordinator1 coordinator service start successfully. pid: 92869 127.0.0.1:22:/data/projects/fedvision started coordinator: port=10000 start coordinator coordinator1 done starting cluster cluster1 clustermanager service start successfully. pid: 92907 127.0.0.1:22:/data/projects/fedvision started cluster manager: port=10001 start cluster cluster1 done, success: True starting cluster workers for cluster cluster1 starting worker worker1 cluster worker service start successfully. pid: 92944 127.0.0.1:22:/data/projects/fedvision started cluster worker: name=worker1 start worker worker1 done, success: True starting master master1 master service start successfully. pid: 92975 127.0.0.1:22:/data/projects/fedvision started master: port=10002 start master master1 done, success: True starting master master2 master service start successfully. pid: 93022 127.0.0.1:22:/data/projects/fedvision started master: port=10003 start master master2 done, success: True starting master master3 master service start successfully. pid: 93067 127.0.0.1:22:/data/projects/fedvision started master: port=10004 start master master3 done, success: True starting master master4 master service start successfully. pid: 93112 127.0.0.1:22:/data/projects/fedvision started master: port=10005 start master master4 done, success: True Run examples \u00b6 Jobs could be submitted at each deployed machine with master service started. $ cd /data/projects/fedvision $ source venv/bin/activate $ export PYTHONPATH = $PYTHONPATH :/data/projects/fedvision/FedVision // submit jobs to master1 $ sh FedVision/examples/paddle_mnist/run.sh 127 .0.0.1:10002 { \"job_id\": \"master1-20201218202835-1\" } Note : find logs in /data/projects/fedvision/FedVision/logs","title":"Quickstart"},{"location":"quickstart/quickstart/#prerequisites","text":"Too run Fedvision, following dependency or tools required: machine to install fedvision-deploy-toolkit: python virtualenv with Python>=3 setup SSH password less login to machine(s) for deploy fedvision framework. machine(s) to deploy fedvision framework: Python>=3.7(with pip) an isolated directory (each directory will be deployed with a copy of code)","title":"Prerequisites"},{"location":"quickstart/quickstart/#deploy","text":"// create and activate python virtual environment $ python3 -V Python 3.9.0 $ python3 -m venv venv && source venv/bin/activate // install fedvision_deploy_toolkit $ ( venv ) python -m pip install -U pip && python -m pip install fedvision_deploy_toolkit ---> 100% Successfully installed fedvision_deploy_toolkit // generate deploy template $ ( venv ) fedvision-deploy template standalone // read comments in generated template standalone_template.yaml` and modify as you want. // deploy now $ ( venv ) fedvision-deploy deploy deploy standalone_template.yaml deploying 2 machines: ['machine1'] ---> 100% deploy done Note Deploying Cluster version is almost same except that you should generate template using fedvision-deploy template template and modify generated template file according to comments.","title":"Deploy"},{"location":"quickstart/quickstart/#services-start","text":"Services could be start/stop with scripts in Fedvision/sbin or, use fedvision deploy toolkits: // start services $ fedvision-deploy services all start standalone_template.yaml staring coordinator coordinator1 coordinator service start successfully. pid: 92869 127.0.0.1:22:/data/projects/fedvision started coordinator: port=10000 start coordinator coordinator1 done starting cluster cluster1 clustermanager service start successfully. pid: 92907 127.0.0.1:22:/data/projects/fedvision started cluster manager: port=10001 start cluster cluster1 done, success: True starting cluster workers for cluster cluster1 starting worker worker1 cluster worker service start successfully. pid: 92944 127.0.0.1:22:/data/projects/fedvision started cluster worker: name=worker1 start worker worker1 done, success: True starting master master1 master service start successfully. pid: 92975 127.0.0.1:22:/data/projects/fedvision started master: port=10002 start master master1 done, success: True starting master master2 master service start successfully. pid: 93022 127.0.0.1:22:/data/projects/fedvision started master: port=10003 start master master2 done, success: True starting master master3 master service start successfully. pid: 93067 127.0.0.1:22:/data/projects/fedvision started master: port=10004 start master master3 done, success: True starting master master4 master service start successfully. pid: 93112 127.0.0.1:22:/data/projects/fedvision started master: port=10005 start master master4 done, success: True","title":"Services start"},{"location":"quickstart/quickstart/#run-examples","text":"Jobs could be submitted at each deployed machine with master service started. $ cd /data/projects/fedvision $ source venv/bin/activate $ export PYTHONPATH = $PYTHONPATH :/data/projects/fedvision/FedVision // submit jobs to master1 $ sh FedVision/examples/paddle_mnist/run.sh 127 .0.0.1:10002 { \"job_id\": \"master1-20201218202835-1\" } Note : find logs in /data/projects/fedvision/FedVision/logs","title":"Run examples"},{"location":"release/change_log/","text":"","title":"Changelog"}]}